<?xml version="1.0" encoding="UTF-8"?>
<entry xmlns="http://www.w3.org/2005/Atom"
       xmlns:openSearch="http://a9.com/-/spec/opensearchrss/1.0/"
       xmlns:gd="http://schemas.google.com/g/2005"
       xmlns:thr="http://purl.org/syndication/thread/1.0"
       xmlns:georss="http://www.georss.org/georss">
   <id>tag:blogger.com,1999:blog-116259103207720939.post-5987860113759830686</id>
   <published>2015-08-18T09:46:00.003-04:00</published>
   <updated>2015-08-18T09:46:41.033-04:00</updated>
   <category scheme="http://schemas.google.com/g/2005#kind"
             term="http://schemas.google.com/blogger/2008/kind#post"/>
   <category scheme="http://www.blogger.com/atom/ns#" term="Digital Library"/>
   <category scheme="http://www.blogger.com/atom/ns#" term="AWOL Internet Archive"/>
   <title type="text">Archiving the AWOL Index</title>
   <content type="html"><![CDATA[<a href="http://ryanfb.github.io/etc/2015/08/18/archiving_the_awol_index.html" rel="bookmark">Archiving the AWOL Index</a><br />Ryan Bauman <br /><blockquote class="tr_bq"><span style="font-size: x-small;">  </span><span style="font-size: x-small;"><a href="http://isaw.nyu.edu/publications/awol-index/">The AWOL Index</a> is a new experimental project to extract structured data from <a href="http://ancientworldonline.blogspot.com/">AWOL - The Ancient World Online</a>, which has published links to material about the ancient world since 2009.</span><br /><br /><span style="font-size: x-small;"> </span><span style="font-size: x-small;">As a practical experiment, I thought it might be interesting to check  which URLs in the index are already in web archives, and try to archive  those which are not. To do this, I downloaded the AWOL index JSON,  unzipped it, and extracted unique linked URLs with:</span><br /><br /><span style="font-size: x-small;"> </span><pre><span style="font-size: x-small;"><code>find . -name '*.json' -exec grep '"url":' {} \; | \<br />sed -e 's/^.*"url": "//' -e 's/".*$//' | \<br />sort -u &gt; urls-clean-uniq.txt</code></span></pre><pre><span style="font-size: x-small;"><code>&nbsp;</code></span></pre><span style="font-size: x-small;"> </span><span style="font-size: x-small;">This gave me <a href="https://gist.github.com/639e9366926c889b112f">52,020 unique URLs</a>.</span><br /><br /><span style="font-size: x-small;"> </span><span style="font-size: x-small;">Initially, I thought it would be best to check if the URLs were in <em>any</em> web archive, rather than just one. To do this, I used the <a href="http://timetravel.mementoweb.org/guide/api/">mementoweb.org Time Travel API</a> to hit the <a href="http://timetravel.mementoweb.org/about/#find">“Find”</a> service to check URL availability in a wide range of archives. Unfortunately, this proved to be a relatively slow process.</span><br /><br /><span style="font-size: x-small;"> </span><span style="font-size: x-small;">In order to speed things up, I decided to try checking and using just one web archive: <a href="http://archive.org/web/">the Internet Archive Wayback Machine</a>. Using some hand-picked URLs that showed as “missing” from the truncated mementoweb.org process, I checked <a href="https://archive.org/help/wayback_api.php">the Wayback Machine Availability API</a> to see what sort of results I got.</span><br /><br /><span style="font-size: x-small;"> </span><span style="font-size: x-small;">Interestingly, this lead to the realization that <a href="http://archive.org/wayback/available?url=http://ager2.voila.net/AGER10.pdf">certain URLs which show no availability in the JSON API</a> <a href="http://web.archive.org/cdx/search/cdx?url=http://ager2.voila.net/AGER10.pdf">do show availability in the CDX API</a>. So, I decided to check URL availability using the relatively fast CDX API for the most accurate results:</span><br /><br /><span style="font-size: x-small;"> </span><pre><span style="font-size: x-small;"><code>while read url; do \<br />  if [ -n "$(curl -s "http://web.archive.org/cdx/search/cdx?url=${url}")" ]; \<br />    then echo "$url" &gt;&gt; cdx-success.txt; \<br />    else echo "$url" &gt;&gt; cdx-failure.txt; \<br />  fi; \<br />done &lt; urls-clean-uniq.txt</code></span></pre><pre><span style="font-size: x-small;"><code>&nbsp;</code></span></pre><span style="font-size: x-small;"> </span><span style="font-size: x-small;">After this process finished running, I had 34,832 URLs showing as  already successfully archived (or about 67%). For the remainder, I  wanted to submit them to the Wayback Machine for archiving, which I did  with:</span><br /><br /><span style="font-size: x-small;"> </span><pre><span style="font-size: x-small;"><code>while read url; do \<br />  echo "$url"; \<br />  curl -L -o/dev/null -s "http://web.archive.org/save/$url"; \<br />done &lt; cdx-failure.txt</code></span></pre><pre><span style="font-size: x-small;"><code>&nbsp;</code></span></pre><span style="font-size: x-small;"> </span><span style="font-size: x-small;">So, any live, savable URLs which weren’t already in the archive at the time this process was run should be added to it.</span><br /><br /><span style="font-size: x-small;"> </span><span style="font-size: x-small;">After this process finished, I did an initial pass at checking the  submitted URLs for presence in the CDX index, and found 10,823 hits for  the 17,188 URLs submitted (a 63% success rate). I also noticed that the  CDX server can <em>occasionally</em> give false negatives as well (i.e.  returns no results for something that’s in the index), so I did another  pass against the 6,365 “missing” URLs to try to see if they were  actually available, which added only 5 URLs as false negatives from the  initial run.</span><br /><br /><span style="font-size: x-small;"> </span><span style="font-size: x-small;">So, after running these processes it seemed the Wayback Machine now  had at least one snapshot for 45,660 of our 52,020 URLs (about 88%).  Spot-checking <a href="https://gist.github.com/5a903d473defe06ac995">the remaining 6,360 URLs</a> showed that some returned no snapshots via <em>either</em> the <a href="http://archive.org/wayback/available?url=http://tobias-lib.uni-tuebingen.de/frontdoor.php?source_opus=5744&amp;la=de">JSON</a> or <a href="http://web.archive.org/cdx/search/cdx?url=http://tobias-lib.uni-tuebingen.de/frontdoor.php?source_opus=5744&amp;la=de">CDX</a> APIs but <a href="http://web.archive.org/web/*/http://tobias-lib.uni-tuebingen.de/frontdoor.php?source_opus=5744&amp;la=de">show snapshots in the web interface</a>. This particular example <a href="http://timetravel.mementoweb.org/api/json/2015/http://tobias-lib.uni-tuebingen.de/frontdoor.php?source_opus=5744&amp;la=de">shows in the mementoweb.org API</a>, so I decided to try checking the Wayback Memento API by hitting <code>http://web.archive.org/web/{URI-R}</code>:</span><br /><br /><span style="font-size: x-small;"> </span><pre><span style="font-size: x-small;"><code>while read url; do echo $url; \<br />  if curl -s --fail -I "http://web.archive.org/web/$url"; \<br />    then echo $url &gt;&gt; memento-success.txt; \<br />  fi; \<br />done &lt; cdx-missing-combined.txt</code></span></pre><pre><span style="font-size: x-small;"><code>&nbsp;</code></span></pre><span style="font-size: x-small;"> </span><span style="font-size: x-small;">This revealed that <a href="https://gist.github.com/bef7166d45f902ee2328">4,606</a> of our 6,360 “missing” URLs were, in fact, successfully archived (so  50,266 of our 52,020 original URLs, or about 97%, now have at least one  snapshot in the Wayback Machine). Looking at <a href="https://gist.github.com/309b5e9a483bd98345c2">the remaining 1,754 missing URLs</a>, we can triage these further and see what currently returns a “live” response code with:</span><br /><br /><span style="font-size: x-small;"> </span><pre><span style="font-size: x-small;"><code>while read url; do \<br />  if curl -s --fail -L -I "$url" ; \<br />    then echo "$url" &gt;&gt; cdx-missing-live-success.txt; \<br />    else echo "$url" &gt;&gt; cdx-missing-live-failure.txt; \<br />  fi; \<br />done &lt; cdx-memento-missing.txt</code></span></pre><pre><span style="font-size: x-small;"><code>&nbsp;</code></span></pre><span style="font-size: x-small;"> </span><span style="font-size: x-small;">Giving us <a href="https://gist.github.com/d73a99e2d1e2b611622a">431 URLs with no snapshots that currently return an HTTP error</a> (so less than 1% of our total URL count).</span><br /><br /><span style="font-size: x-small;"> </span><span style="font-size: x-small;">I plan on doing one more archive run for <a href="https://gist.github.com/8629e20583481082bd00">the remaining 1,323 missing URLs</a>, just in case some temporary server issues cropped up during the initial run.</span><br /><br /><span style="font-size: x-small;"> </span><span style="font-size: x-small;">Going forward, it might be helpful to automate this process to check  and archive new URLs in the AWOL Index on a periodic basis. There are  probably much more interesting things that can be done with mining and  analyzing the AWOL Index, but the foundation of some of these activities  will rely on the simple availability of the linked content.</span><br /><br /><span style="font-size: x-small;">  </span><div id="footer">    <span style="font-size: x-small;"><span>Originally published on 2015-08-18 by <a href="https://ryanfb.github.io/">Ryan Baumann</a></span><span style="float: right;">Feedback? <a href="mailto:ryanfb@gmail.com">e-mail</a> / <a href="https://twitter.com/intent/tweet?text=%40ryanfb%20">twitter</a> / <a href="https://github.com/ryanfb/etc/issues">github</a></span><br /><span>&nbsp;</span></span></div><div id="footer"><span style="font-size: x-small;"><span><a href="https://github.com/ryanfb/etc/commits/gh-pages/_posts/2015-08-18-archiving_the_awol_index.md">Revision History</a></span><br /></span></div><div id="footer"><span style="font-size: x-small;"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width: 0px;" /></a><br />&nbsp;</span></div><div id="footer"><span style="font-size: x-small;">This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>.  </span> </div></blockquote>]]></content>
   <link rel="replies"
         type="application/atom+xml"
         href="https://ancientworldonline.blogspot.com/feeds/5987860113759830686/comments/default"
         title="Post Comments"/>
   <link rel="replies"
         type="text/html"
         href="http://ancientworldonline.blogspot.com/2015/08/archiving-awol-index.html#comment-form"
         title="0 Comments"/>
   <link rel="edit"
         type="application/atom+xml"
         href="https://www.blogger.com/feeds/116259103207720939/posts/default/5987860113759830686"/>
   <link rel="self"
         type="application/atom+xml"
         href="https://www.blogger.com/feeds/116259103207720939/posts/default/5987860113759830686"/>
   <link rel="alternate"
         type="text/html"
         href="http://ancientworldonline.blogspot.com/2015/08/archiving-awol-index.html"
         title="Archiving the AWOL Index"/>
   <author>
      <name>Charles Jones</name>
      <uri>https://plus.google.com/114326413909322730653</uri>
      <email>noreply@blogger.com</email>
   </author>
</entry>
